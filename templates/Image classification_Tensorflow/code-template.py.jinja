# Before running, install required packages:
{% if notebook %}

!
{%- else %}
#
{%- endif %}
 pip install numpy tensorflow{% if visualization_tool == "comet.ml" %} comet_ml{% endif %}

{% if notebook %}


# ---
{% endif %}

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import img_to_array, array_to_img
from tensorflow.keras.layers import GlobalAveragePooling2D
{% if data_format == "Image files" %}
import urllib
import zipfile
{% endif %}
{% if visualization_tool == "Tensorboard" or checkpoint %}
from datetime import datetime
{% endif %}
{% if checkpoint %}
from pathlib import Path
{% endif %}

{% if data_format == "Numpy arrays" %}
def fake_data():
    # 4 images of shape 1x16x16 with labels 0, 1, 2, 3
    return [np.random.rand(4, 1, 16, 16), np.arange(4)]

{% elif data_format == "Image files" %}


# COMMENT THIS OUT IF YOU USE YOUR OWN DATA.
# Download example data into ./data/image-data (4 image files, 2 for "dog", 2 for "cat").
url = "https://github.com/jrieke/traingenerator/raw/main/data/fake-image-data.zip"
zip_path, _ = urllib.request.urlretrieve(url)
with zipfile.ZipFile(zip_path, "r") as f:
    f.extractall("data")

{% endif %}

{{ header("Setup") }}
{% if data_format == "Numpy arrays" %}
# INSERT YOUR DATA HERE
# Expected format: [images, labels]
# - images has array shape (num samples, color channels, height, width)
# - labels has array shape (num samples, )
train_data = fake_data()  # required
val_data = fake_data()    # optional
test_data = None          # optional
{% elif data_format == "Image files" %}
# INSERT YOUR DATA HERE
# Expected format: One folder per class, e.g.
# train
# --- dogs
# |   +-- lassie.jpg
# |   +-- komissar-rex.png
# --- cats
# |   +-- garfield.png
# |   +-- smelly-cat.png
#
# Example: https://github.com/jrieke/traingenerator/tree/main/data/image-data
train_data = "data/image-data"  # required
val_data = "data/image-data"    # optional
test_data = None                # optional
{% endif %}

# Set up hyperparameters.
lr = {{ lr }}
batch_size = {{ batch_size }}
num_epochs = {{ num_epochs}}

{# TODO Add Image_Size #}
img_size = (160,160)
img_shape = img_size + (3,)

# Set up logging.

{% if visualization_tool == "Tensorboard" or checkpoint %}
experiment_id = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
{% endif %}
{% if visualization_tool == "Tensorboard" %}
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=experiment_id, histogram_freq=1)
{% endif %}
{% if visualization_tool == "comet.ml" %}
experiment = Experiment("{{ comet_api_key }}"{% if comet_project %}, project_name="{{ comet_project }}"{% endif %})
{% endif %}
{% if checkpoint %}
checkpoint_dir = tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints/{experiment_id}/model.{epoch:02d}-{val_loss:.2f}.h5')
{% endif %}
print_every = {{ print_every }}  # batches

{{ header("Preprocessing") }}
def preprocess(data, name):
    if data is None:  # val/test can be empty
        return None
        
    {% if data_format == "Image files" %}
    # Read image files to tensorflow dataset.
    dataset = image_dataset_from_directory(data,
                                           shuffle=(name=="train"),
                                           image_size=img_size)
                                          

    {% elif data_format == "Numpy arrays" %}
    images, labels = data

    # Rescale images to 0-255 and convert to uint8.
    # Note: This is done for each dataset individually, which is usually ok if all 
    # datasets look similar. If not, scale all datasets based on min/ptp of train set.
    images = (images - np.min(images)) / np.ptp(images) * 255
    images = images.astype(np.uint8)

    # If images are grayscale, convert to RGB by duplicating channels.
    if images.shape[1] == 1:
        images = np.stack((images[:, 0],) * 3, axis=1)
    

    images = np.rollaxis(images, 1, 4) # Reshape Image to channels_last

    images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((256,256))) for im in images])
    
    dataset = images, labels
    {% endif %}
    return dataset

train_loader = preprocess(train_data, "train")
val_loader = preprocess(val_data, "val")
test_loader = preprocess(test_data, "test")

{{ header("data augmentation") }}
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
])

{{ header("Model") }}
# Create the base model
base_model = tf.keras.applications.{{model_func}}(input_shape=img_shape,
                                               {% if num_classes != 1000 %}
                                               include_top = False,
                                               {% else %}
                                               include_top = True,
                                               {% endif %}
                                               {% if pretrained %}
                                               weights="{{ pretrained }},"
                                               {% else %}
                                               weights = None,
                                               {% endif %}
                                               )



# using the Keras Functional API
{% if num_classes != 1000 %}
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense({{num_classes}})

inputs = tf.keras.Input(shape=img_shape)
x = data_augmentation(inputs)
x = tf.keras.applications.{{model_pre}}.preprocess_input(x)
x = base_model(x)
x = global_average_layer(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

{% else %}
inputs = tf.keras.Input(shape=img_shape)
x = data_augmentation(inputs)
x = tf.keras.applications.{{model_pre}}.preprocess_input(x)
outputs = base_model(x)
model = tf.keras.Model(inputs, outputs)
{% endif %}

model.compile(optimizer = tf.keras.optimizers.{{ optimizer }}(lr={{lr}}),
              loss = "{{ loss }}",
              metrics = ["accuracy"])

model.fit(train_loader,
        batch_size={{batch_size}},
        epochs={{num_epochs}},
        validation_data=val_loader,
        {% if visualization_tool == "Tensorboard" and checkpoint%}
        callbacks = [tensorboard_callback, checkpoint_dir],
        {% elif checkpoint %}
        callbacks = [checkpoint_dir],
        {% elif visualization_tool == "Tensorboard" %}
        callbacks = [tensorboard_callback],
        {% endif %}
        )

model.evaluate(val_loader)